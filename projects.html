<!DOCTYPE html>
<html lang="en">
<head>
	<title>Yixiao Ge</title>

	<!-- Meta -->
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author" content="Xiaoying Riley at 3rd Wave Media">
	<link rel="shortcut icon" href="images/favicon.ico">

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900" rel="stylesheet">

	<!-- FontAwesome JS-->
	<script defer src="assets/fontawesome/js/all.min.js"></script>

	<!-- Theme CSS -->
	<link id="theme-style" rel="stylesheet" href="assets/css/devresume.css">
	<link rel="stylesheet" href="css/font-awesome.min.css">

</head>

<body>
	<!-- DEMO ONLY -->
	<div class="main-wrapper">
		<div class="container px-3 px-lg-5">
			<article class="resume-wrapper mx-auto theme-bg-light p-5 mb-5 my-5 shadow-lg">

				<div class="resume-header">
					<div class="row align-items-center">
						<div class="resume-title col-12 col-md-6 col-lg-8 col-xl-9">
							<h1><font face="verdana">Yixiao Ge</font></h1>
							<!-- <div class="resume-tagline mb-3 mb-md-0">Researcher @ Tencent ARC Lab & Tencent AI Lab</div> -->
						</div><!--//resume-title-->
						<div class="resume-contact col-12 col-md-6 col-lg-4 col-xl-3">
							<ul class="list-unstyled mb-0">
								<li class="mb-2"><i class="fas fa-envelope-square fa-fw fa-lg mr-2"></i><a class="resume-link" href="mailto:geyixiao831@gmail.com">geyixiao831@gmail.com</a></li>
								<li class="mb-2"><i class="fab fa-google fa-fw fa-lg mr-2"></i><a class="resume-link" href="https://scholar.google.com/citations?user=TtU74NAAAAAJ&hl=en">Google Scholar</a></li>
								<li class="mb-2"><i class="fas fab fa-github fa-fw fa-lg mr-2 "></i><a class="resume-link" href="https://github.com/yxgeee">Github</a></li>
								<li class="mb-2"><i class="fas fab fa-twitter fa-fw fa-lg mr-2 "></i><a class="resume-link" href="https://twitter.com/ge_yixiao">X (Twitter)</a></li>
								<!-- <li class="mb-0"><i class="fas fa-map-marker-alt fa-fw fa-lg mr-2"></i>Beijing, China</li> -->
							</ul>
						</div><!--//resume-contact-->
					</div><!--//row-->

				</div><!--//resume-header-->
				<hr>
				<div class="resume-intro py-3">
					<div class="media flex-column flex-md-row align-items-center">
						<img class="resume-profile-image mb-3 mb-md-0 mr-md-5 ml-md-0 rounded mx-auto" src="images/yxge_2023.JPG" alt="image">
						<div class="media-body text-left">
							<p class="mb-0">
								I am currently a senior researcher at <a href="https://arc.tencent.com/en/index" target="_blank">Tencent ARC Lab</a> and <a href="https://ai.tencent.com/ailab/en/index" target="_blank">Tencent AI Lab</a>, leading an effort on <b>multimodal foundation models, open-world visual comprehension, and efficient AI</b>.
				                Previously, I got my Ph.D. degree from <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">Multimedia Lab (MMLab)</a>, <a href="http://www.cuhk.edu.hk/english/index.html" target="_blank">the Chinese University of Hong Kong</a>.

				                <!-- advised by <a href="http://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Prof. Hongsheng Li</a> and <a href="http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Prof. Xiaogang Wang</a>. -->
				                <!-- Previously, I received the B.Eng. degree from <a href="http://english.hust.edu.cn/" target="_blank">Huazhong University of Science and Technology</a>. -->
				                 <!-- in 2017. -->
				                <!-- My research interests include computer vision and deep learning with focus on foundation models and vision+language. -->
				                <!-- large-scale pre-training, un/self-/semi-/weakly-supervised learning, image/video/cross-modality representation learning, transfer learning, etc. -->
								<!-- <font color="#3366CC">[<a href="https://scholar.google.com/citations?user=TtU74NAAAAAJ&hl=en">Google Scholar</a>]</font> -->
								<br><br>
							<!-- </p><br> -->
							<!-- <p> -->
								<font color="#CC0033">Actively looking for self-motivated interns to work on related research topics. Feel free to reach out if you are interested.</font>
								<!-- <br><br> -->
							</p>
						</div><!--//media-body-->
					</div>
				</div><!--//resume-intro-->
				<hr>
				<div class="resume-body">
					<div class="row">
						<div class="resume-main col-12">

								<section class="work-section py-3">
									<h3 class="text-uppercase resume-section-heading mb-4">Projects <a href='./index.html'>[Back]</a></h3>
									<div class="item mb-3">
										<div class="item-content">
											<p>
												<font color="#39AB56"><b>Multimodal Foundation Models:</b></font>
												<!-- <br> -->
												<ul>
													<li>
														<p><b>Vision-language:</b>
														We aim to develop foundational models that unify visual comprehension and generation tasks within one framework. </p>
														<p>Given the great success of Large Language Models (LLMs), we take the initial step to empower the off-the-shelf LLMs with the ability to perform visual tasks via plugins (<a href = "https://gpt4tools.github.io/" target="_blank">GPT4Tools @NeurIPS23</a>). Despite a feasible solution, it is far from multimodal emergent abilities. </p>
														<p>We are further devoted to developing an end-to-end framework that facilitates flexible input/output formats, transitioning and reasoning seamlessly between multimodal signals while acquiring knowledge from an inherently multimodal world. Check out our <a href = "https://ailab-cvc.github.io/seed/" target="_blank"><b>SEED</b></a> for details. </p>
														<p>Previously, we focused on pre-training vision-language representations and video-text retrieval, e.g., <a href="https://arxiv.org/abs/2201.04850" target="_blank">MCQ @CVPR22(Oral)</a>, <a href="https://arxiv.org/abs/2203.07303" target="_blank">All-in-One @CVPR23</a>. We also made some interesting applications like <a href="https://tuneavideo.github.io/" target="_blank">Tune-A-Video @ICCV23</a>.</p>
													</li>
													<li>
														<p><b>Omni-modal:</b>
														A real AI agent (e.g., a smart robot) should be capable of sensing all modalities. It is non-trivial, especially for those rare modalities. Check out our solution, namely, <a href = "https://github.com/TencentARC/ViT-Lens" target="_blank">ViT-Lens</a>. Omni-modal representation has great potential in emergent applications, see our <a href = "https://arxiv.org/abs/2306.16934" target="_blank">DreamDiffusion</a>.</p>
													</li>
													<li>
														<p><b>Data-centric:</b>
														High-quality and large-scale data is the prerequisite for training foundation models. For training data, we collect large-scale TV dramas (<a href = "https://ptvd.github.io/#" target="_blank">PTVD</a>, Tencent Video authorization), as well as memes (<a href = "https://arxiv.org/abs/2306.06870" target="_blank">Sticker820K</a>, Tencent Search authorization). Besides, we are also focusing on properly evaluating multimodal LLMs, proposing <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a> (<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard" target="_blank">[leaderboard]</a>).
														</p>
													</li>
												</ul>
											</p>
											<p>
												<font color="#39AB56"><b>Open-world Visual Comprehension:</b></font>
												<ul>
													<li>
														<p><b>Visual representation:</b>
														We are committed to improving image representation (e.g., <a href = "https://arxiv.org/abs/2203.15371" target="_blank">mc-BEiT @ECCV22</a>, <a href = "https://openreview.net/pdf?id=1fZd4owfJP6" target="_blank">ConMIM @ICLR23</a>, <a href = "https://arxiv.org/abs/2301.06958" target="_blank">RILS @CVPR23</a>) and video representation (e.g., <a href = "https://arxiv.org/abs/2209.15280" target="_blank">TVTS @CVPR23</a>, <a href = "https://arxiv.org/abs/2305.14173" target="_blank">TVTSv2</a>) via large-scale pre-training.
														</p>
													</li>
													<li>
														<p><b>Visual perception:</b>
														We also tackle the challenge of visual perception tasks, for instance, detection and segmentation. Check out our <a href = "https://arxiv.org/abs/2204.02964" target="_blank">MIMDet @ICCV23</a>, <a href = "https://arxiv.org/abs/2303.11630" target="_blank">BoxSnake @ICCV23</a>.</p>
													</li>
												</ul>
											</p>
											<p>
												<font color="#39AB56"><b>Efficient AI:</b></font>
												<ul>
													<p>
														<!-- Efficient (green) AI is an important task in industry.  -->
														We have created a new topic of hot-refresh <b>model upgrades</b> (<a href = "https://openreview.net/pdf?id=HTp-6yLGGX" target="_blank">RACT @ICLR22</a>) for large-scale retrieval systems, which is practical in industry and under-explored in academia. Beyond retrieval, upgrading the foundation models in current AI systems is also costly because all downstream modules need to be retrained to adapt. Check out our <a href = "https://arxiv.org/abs/2306.12642" target="_blank">TaCA</a> for a solution.
														We are also interested in <b>model selection</b> (<a href = "https://arxiv.org/abs/2207.03036" target="_blank">SFDA @ECCV22</a>, <a href = "https://arxiv.org/abs/2308.15074" target="_blank">PED @ICCV23</a>), <b>binarization</b> (<a href = "https://arxiv.org/abs/2302.08714" target="_blank">BEBR @KDD23</a>), etc.
													</p>
													<p>
														Our algorithms helped Tencent effectively reduce costs and increase efficiency. We won the highest technical award within the company and the <a href="https://www.szccf.org.cn/?p=4198" target="_blank">SZCCF Science and Technology Award</a>.
													</p>
												</ul>
											</p>
										</div>
									</div><!--//item-->

								</section><!--//work-section-->
						</div><!--//resume-main-->

							</div><!--//row-->
						</div><!--//resume-body-->

					</article>

				</div><!--//container-->

				<!-- <footer class="footer text-center py-4">
					&lt;!&ndash;/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */&ndash;&gt;
					<small class="copyright text-muted">Designed with <i class="fas fa-heart"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small>
				</footer> -->

			</div><!--//main-wrapper-->


</body>
</html>
