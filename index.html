<!DOCTYPE html>
<html lang="en">
<head>
	<title>Yixiao Ge</title>

	<!-- Meta -->
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author" content="Xiaoying Riley at 3rd Wave Media">
	<link rel="shortcut icon" href="images/favicon.ico">

	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900" rel="stylesheet">

	<!-- FontAwesome JS-->
	<script defer src="assets/fontawesome/js/all.min.js"></script>

	<!-- Theme CSS -->
	<link id="theme-style" rel="stylesheet" href="assets/css/devresume.css">
	<link rel="stylesheet" href="css/font-awesome.min.css">

</head>

<body>
	<!-- DEMO ONLY -->
	<div class="main-wrapper">
		<div class="container px-3 px-lg-5">
			<article class="resume-wrapper mx-auto theme-bg-light p-5 mb-5 my-5 shadow-lg">

				<div class="resume-header">
					<div class="row align-items-center">
						<div class="resume-title col-12 col-md-6 col-lg-8 col-xl-9">
							<h1><font face="verdana">Yixiao Ge</font></h1>
							<!-- <div class="resume-tagline mb-3 mb-md-0">Researcher @ Tencent ARC Lab & Tencent AI Lab</div> -->
						</div><!--//resume-title-->
						<div class="resume-contact col-12 col-md-6 col-lg-4 col-xl-3">
							<ul class="list-unstyled mb-0">
								<li class="mb-2"><i class="fas fa-envelope-square fa-fw fa-lg mr-2"></i><a class="resume-link" href="mailto:geyixiao831@gmail.com">geyixiao831@gmail.com</a></li>
								<li class="mb-2"><i class="fab fa-google fa-fw fa-lg mr-2"></i><a class="resume-link" href="https://scholar.google.com/citations?user=TtU74NAAAAAJ&hl=en">Google Scholar</a></li>
								<li class="mb-2"><i class="fas fab fa-github fa-fw fa-lg mr-2 "></i><a class="resume-link" href="https://github.com/yxgeee">Github</a></li>
								<li class="mb-0"><i class="fas fa-map-marker-alt fa-fw fa-lg mr-2"></i>Beijing, China</li>
							</ul>
						</div><!--//resume-contact-->
					</div><!--//row-->

				</div><!--//resume-header-->
				<hr>
				<div class="resume-intro py-3">
					<div class="media flex-column flex-md-row align-items-center">
						<img class="resume-profile-image mb-3 mb-md-0 mr-md-5 ml-md-0 rounded mx-auto" src="images/yxge_2023.JPG" alt="image">
						<div class="media-body text-left">
							<p class="mb-0">
								I am currently a senior researcher at <a href="https://arc.tencent.com/en/index" target="_blank">Tencent ARC Lab</a> and <a href="https://ai.tencent.com/ailab/en/index" target="_blank">Tencent AI Lab</a>, leading an effort on <b>multimodal foundation models, open-world visual comprehension, and efficient AI</b>.
				                Previously, I got my Ph.D. degree from <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">Multimedia Lab (MMLab)</a>, <a href="http://www.cuhk.edu.hk/english/index.html" target="_blank">the Chinese University of Hong Kong</a>,
				                advised by <a href="http://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Prof. Hongsheng Li</a> and <a href="http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Prof. Xiaogang Wang</a>.
				                <!-- Previously, I received the B.Eng. degree from <a href="http://english.hust.edu.cn/" target="_blank">Huazhong University of Science and Technology</a>. -->
				                 <!-- in 2017. -->
				                <!-- My research interests include computer vision and deep learning with focus on foundation models and vision+language. -->
				                <!-- large-scale pre-training, un/self-/semi-/weakly-supervised learning, image/video/cross-modality representation learning, transfer learning, etc. -->
								<!-- <font color="#3366CC">[<a href="https://scholar.google.com/citations?user=TtU74NAAAAAJ&hl=en">Google Scholar</a>]</font> -->
								<!-- <br> -->
							<!-- </p><br> -->
							<!-- <p> -->
								<font color="#CC0033">We are actively looking for self-motivated interns to work on related research topics. Please feel free to reach out if you are interested.</font>
								<!-- <br><br> -->
							</p>
						</div><!--//media-body-->
					</div>
				</div><!--//resume-intro-->
				<hr>
				<div class="resume-body">
					<div class="row">
						<div class="resume-main col-12 col-lg-8 col-xl-9 pr-0 pr-lg-5">
							<section class="work-section py-3">
								<h3 class="text-uppercase resume-section-heading mb-4">News: </h3>
								
								<div class="item mb-3">
									<div class="item-content">
										<p>
											<b><span class="css-rainbow-text">Welcome to check out our SEED</span> (<a href = "https://ailab-cvc.github.io/seed/" target="_blank">[Project Page]</a>)!</b>
										</p>
										<ul class="resume-list" style="list-style: outside;list-style-type: square;">
											<li> <b>[Oct 2023]</b> Excited to unveil <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED-LLaMA</a>, featuring multi-turn in-context emergent capabilities.</li>
											<li> <b>[Sep 2023]</b> Three papers are accepted to NeurIPS 2023.</li>
											<li> <b>[Aug 2023]</b> Glad to release <a href = "https://github.com/TencentARC/ViT-Lens" target="_blank">ViT-Lens</a>, advancing omni-modal representation learning.</li>
											<li> <b>[Aug 2023]</b> Glad to release <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a>, the most comprehensive MLLM benchmark to date.</li>
											<li> <b>[July 2023]</b> Glad to release <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED</a>, an image tokenizer tailored for LLM.</li>
											<li> <b>[July 2023]</b> Four papers are accepted to ICCV 2023.</li>
											<li> <b>[May 2023]</b> One paper is accepted to KDD 2023.</li>
											<li> <b>[Apr 2023]</b> One paper is accepted to ICML 2023.</li>
											<!-- <li> <b>[Apr 2023]</b> We release several interesting projects towards generative comprehension: <font color="#CC0033">TagGPT, VLog, and GPT4Tools</font>. Welcome to check them out!</li> -->
											<li> <b>[Feb 2023]</b> Four papers are accepted to CVPR 2023.</li>
											<li> <b>[Jan 2023]</b> One paper is accepted to ICLR 2023.</li>
											<br>
											<li> <b>[Jan-Nov 2022]</b> 11 papers were accepted by ICLR/CVPR/IJCAI/ECCV 2022 and AAAI 2023, 2 of which were oral.</li>
											<li> <b>[Mar-Jul 2021]</b> 5 papers were accepted by CVPR/ICCV 2021.</li>
											<li> <b>[Jan-Sep 2020]</b> 3 papers were accepted by ICLR/ECCV/NeurIPS 2020, 1 of which was spotlight.</li>
											<!-- <li> <b>[Nov 2022]</b> Two papers are accepted to AAAI 2023.</li>
											<li> <b>[Jul 2022]</b> Three papers are accepted to ECCV 2022.</li>
											<li> <b>[Apr 2022]</b> One paper is accepted to IJCAI 2022 as a <font color="#CC0033">Long oral</font> presentation.</li>
											<li> <b>[Mar 2022]</b> Two papers are accepted to CVPR 2022 with one <font color="#CC0033">Oral</font> presentation.</li>
											<li> <b>[Jan 2022]</b> Three papers are accepted to ICLR 2022.</li> -->
											<!-- <li> <b>[Jul 2021]</b> Two papers are accepted to ICCV 2021.</li> -->
											<!-- <li> <b>[Mar 2021]</b> Three papers are accepted to CVPR 2021.</li> -->
											<!-- <li> <b>[Sep 2020]</b> One paper is accepted to NeurIPS 2020.</li> -->
											<!-- <li> <b>[Aug 2020]</b> We achieved 2nd place in <a href='http://ai.bu.edu/visda-2020/'>VisDA-2020 Challenge</a> in ECCV 2020. <a href='https://geyixiao.com/projects/visda.html'>[Project]</a> </li> -->
												<!-- <a href='https://arxiv.org/abs/2008.10313'>[pdf]</a> <a href='https://github.com/yxgeee/MMT-plus'>[code]</a></li> -->
											<!-- <li> <b>[Jul 2020]</b> One paper is accepted to ECCV 2020 as a <font color="#CC0033">Spotlight</font> presentation.</li> -->
											<!-- <li><b>[Jul 2020]</b> The OpenUnReID codebase is online. <a href='https://github.com/open-mmlab/OpenUnReID'>[Code]</a></li> -->
											<!-- <li> <b>[Dec 2019]</b> One paper is accepted to ICLR 2020.</li>
											<li> <b>[Sep 2018]</b> One paper is accepted to NeurIPS 2018.</li> -->
										</ul>
									</div>
								</div><!--//item-->

							</section><!--//work-section-->

							<section class="work-section py-3">
								<h3 class="text-uppercase resume-section-heading mb-4">Selected Projects</h3>
								<div class="item mb-3">
									<div class="item-content">
										<p>
											<font color="#39AB56"><b>Multimodal Foundation Models:</b></font>
											<!-- <br> -->
											<ul>
												<li>
													<p><b>Vision-language:</b>
													We aim to develop foundational models that unify visual comprehension and generation tasks within one framework. </p>
													<p>Given the great success of Large Language Models (LLMs), we take the initial step to empower the off-the-shelf LLMs with the ability to perform visual tasks via plugins (<a href = "https://gpt4tools.github.io/" target="_blank">GPT4Tools @NeurIPS23</a>). Despite a feasible solution, it is far from multimodal emergent abilities. </p>
													<p>We are further devoted to developing an end-to-end framework that facilitates flexible input/output formats, transitioning and reasoning seamlessly between multimodal signals while acquiring knowledge from an inherently multimodal world. Check out our <a href = "https://ailab-cvc.github.io/seed/" target="_blank"><b>SEED</b></a> for details. </p>
													<p>Previously, we focused on pre-training vision-language representations and video-text retrieval, e.g., <a href="https://arxiv.org/abs/2201.04850" target="_blank">MCQ @CVPR22(Oral)</a>, <a href="https://arxiv.org/abs/2203.07303" target="_blank">All-in-One @CVPR23</a>. We also made some interesting applications like <a href="https://tuneavideo.github.io/" target="_blank">Tune-A-Video @ICCV23</a>.</p>
												</li>
												<li>
													<p><b>Omni-modal:</b>
													A real AI agent (e.g., a smart robot) should be capable of sensing all modalities. It is non-trivial, especially for those rare modalities. Check out our solution, namely, <a href = "https://arxiv.org/abs/2308.10185" target="_blank">ViT-Lens</a>. Omni-modal representation has great potential in emergent applications, see our <a href = "https://arxiv.org/abs/2306.16934" target="_blank">DreamDiffusion</a>.</p>
												</li>
												<li>
													<p><b>Data-centric:</b>
													High-quality and large-scale data is the prerequisite for training foundation models. For training data, we collect large-scale TV dramas (<a href = "https://ptvd.github.io/#" target="_blank">PTVD</a>, Tencent Video authorization), as well as memes (<a href = "https://arxiv.org/abs/2306.06870" target="_blank">Sticker820K</a>, Tencent Search authorization). Besides, we are also focusing on properly evaluating multimodal LLMs, proposing <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a> (<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard" target="_blank">[leaderboard]</a>).
													</p>
												</li>
											</ul>
										</p>
										<p>
											<font color="#39AB56"><b>Open-world Visual Comprehension:</b></font>
											<ul>
												<li>
													<p><b>Visual representation:</b>
													We are committed to improving image representation (e.g., <a href = "https://arxiv.org/abs/2203.15371" target="_blank">mc-BEiT @ECCV22</a>, <a href = "https://openreview.net/pdf?id=1fZd4owfJP6" target="_blank">ConMIM @ICLR23</a>, <a href = "https://arxiv.org/abs/2301.06958" target="_blank">RILS @CVPR23</a>) and video representation (e.g., <a href = "https://arxiv.org/abs/2209.15280" target="_blank">TVTS @CVPR23</a>, <a href = "https://arxiv.org/abs/2305.14173" target="_blank">TVTSv2</a>) via large-scale pre-training.
													</p>
												</li>
												<li>
													<p><b>Visual perception:</b>
													We also tackle the challenge of visual perception tasks, for instance, detection and segmentation. Check out our <a href = "https://arxiv.org/abs/2204.02964" target="_blank">MIMDet @ICCV23</a>, <a href = "https://arxiv.org/abs/2303.11630" target="_blank">BoxSnake @ICCV23</a>.</p>
												</li>
											</ul>
										</p>
										<p>
											<font color="#39AB56"><b>Efficient AI:</b></font>
											<ul>
												<p>
													<!-- Efficient (green) AI is an important task in industry.  -->
													We have created a new topic of hot-refresh <b>model upgrades</b> (<a href = "https://openreview.net/pdf?id=HTp-6yLGGX" target="_blank">RACT @ICLR22</a>) for large-scale retrieval systems, which is practical in industry and under-explored in academia. Beyond retrieval, upgrading the foundation models in current AI systems is also costly because all downstream modules need to be retrained to adapt. Check out our <a href = "https://arxiv.org/abs/2306.12642" target="_blank">TaCA</a> for a solution.
													We are also interested in <b>model selection</b> (<a href = "https://arxiv.org/abs/2207.03036" target="_blank">SFDA @ECCV22</a>, <a href = "https://arxiv.org/abs/2308.15074" target="_blank">PED @ICCV23</a>), <b>binarization</b> (<a href = "https://arxiv.org/abs/2302.08714" target="_blank">BEBR @KDD23</a>), etc. 
												</p>
												<p>
													Our algorithms helped Tencent effectively reduce costs and increase efficiency. We won the highest technical award within the company and the <a href="https://www.szccf.org.cn/?p=4198" target="_blank">SZCCF Science and Technology Award</a>.
												</p>
											</ul>
										</p>
									</div>
								</div><!--//item-->

							</section><!--//work-section-->


							<section class="project-section py-3">
								<h3 class="text-uppercase resume-section-heading mb-4">Publications <a href='https://scholar.google.com/citations?user=TtU74NAAAAAJ&hl=en'>[Full List]</a></h3>

								<div class="item mb-3">
									<div class="item-content">
										<!-- <font color="#39AB56"><b>Selected Preprints:</b></font>
										<ul class="resume-list" style="list-style: outside;" >

										</ul> -->
										<font color="#39AB56">
											<b>(&nbsp;*equal contribution &nbsp; <sup>#</sup>corresponding author&nbsp;)
											<!-- <sup><span lang="EN-US" style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span></sup>corresponding author) -->
											</b>
										</font>
										<br>
										<br>
										<p>
										<font color="#39AB56"><b>Selected Preprints:</b></font>
										</p>
										<ul class="resume-list" style="list-style: outside;" >

											<li>
												<div class="resume-degree font-weight-bold">
													Making LLaMA SEE and Draw with SEED Tokenizer
												</div>
												<font color="#CC0033">
													Offers unified multimodal comprehension and generation, featuring multi-turn in-context emergent capabilities, akin to an AI aide.
												</font>
												<div class="resume-degree-org text-muted">
													Yuying Ge*, Sijie Zhao*, Ziyun Zeng, <b>Yixiao Ge<sup>#</sup></b>, Chen Li, Xintao Wang, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://ailab-cvc.github.io/seed/seed_llama.html" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2310.01218" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/AILab-CVC/SEED" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Planting a SEED of Vision in Large Language Model
												</div>
												<font color="#CC0033">
													Empowers Large Language Models (LLMs) with the emergent ability to see and draw.
												</font>
												<div class="resume-degree-org text-muted">
													Yuying Ge*, <b>Yixiao Ge*<sup>#</sup></b>, Ziyun Zeng, Xintao Wang, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://ailab-cvc.github.io/seed/seed.html" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2307.08041" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/AILab-CVC/SEED" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													ViT-Lens: Towards Omni-modal Representations
												</div>
												<font color="#CC0033">
													Advancing omni-modal representation learning with modality lens.
												</font>
												<div class="resume-degree-org text-muted">
													Weixian Lei, <b>Yixiao Ge<sup>#</sup></b>, Jianfeng Zhang, Dylan Sun, Kun Yi, Ying Shan, Mike Zheng Shou<sup>#</sup>
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://arxiv.org/abs/2308.10185" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/TencentARC/ViT-Lens" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/ViT-Lens?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension
												</div>
												<font color="#CC0033">
													Consists of 19K multiple-choice questions with accurate human annotations, spans 12 evaluation dimensions in terms of both spatial and temporal comprehension.
												</font>
												<div class="resume-degree-org text-muted">
													Bohao Li*, Rui Wang*, Guangzhi Wang*, Yuying Ge<sup>#</sup>, <b>Yixiao Ge<sup>#</sup></b>, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://arxiv.org/abs/2307.16125" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter
												</div>
												<font color="#CC0033">
													Enabling new ViTs plugged into the framework (e.g., BLIP-2)  with other modules untouched and a performance boost.
												</font>
												<div class="resume-degree-org text-muted">
													Binjie Zhang, <b>Yixiao Ge<sup>#</sup></b>, Xuyuan Xu, Ying Shan, Mike Zheng Shou<sup>#</sup>
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://arxiv.org/abs/2306.12642" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/TencentARC/TaCA" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TaCA?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													What Makes for Good Visual Tokenizers for Large Language Models?
												</div>
												<font color="#CC0033">
													Rather than simply applying CLIP models, we systematically investigate proper pre-training methods to build good visual tokenizers, making LLMs powerful multimodal LLMs.
												</font>
												<div class="resume-degree-org text-muted">
													Guangzhi Wang, <b>Yixiao Ge<sup>#</sup></b>, Xiaohan Ding, Mohan Kankanhalli, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://arxiv.org/abs/2305.12223" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/TencentARC/GVT" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/GVT?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale
												</div>
												<font color="#CC0033">
													Producing general-purpose video features that work out of the box. We surpass InternVideo and ImageBind on zero-shot and linear tasks.
												</font>
												<div class="resume-degree-org text-muted">
													Ziyun Zeng, <b>Yixiao Ge<sup>#</sup></b>, Zhan Tong, Xihui Liu, Shu-Tao Xia, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													[<a href = "https://arxiv.org/abs/2305.14173" target="_blank">Tech Report</a>]
													[<a href = "https://github.com/TencentARC/TVTS" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TVTS?style=social">
												</div>
											</li>


									<!-- <li>
										<div class="resume-degree font-weight-bold">
											VLog: Video as a Long Document
										</div>
										<font color="#CC0033">
											Given a long video, we turn it into a document containing visual + audio info. By sending this document to ChatGPT, we can chat over the video!
										</font>
										<div class="resume-degree-org text-muted">
											[<a href = "https://huggingface.co/spaces/TencentARC/VLog" target="_blank">Demo</a>]
											[<a href = "https://github.com/showlab/VLog" target="_blank">Code</a>]
											<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/showlab/VLog?style=social">
										</div>
									</li> -->
									<!-- <li>
										<div class="resume-degree font-weight-bold">
											Caption Anything: Interactive Image Description with Diverse Multimodal Controls
										</div>
										<font color="#CC0033">
											We generate descriptive captions for any object within an image, offering a range of language styles to accommodate diverse user preferences. It supports visual controls (mouse click) and language controls (length, sentiment, factuality, and language).
										</font>
										<div class="resume-degree-org text-muted">
											Teng Wang*, Jinrui Zhang*, Junjie Fei*, <b>Yixiao Ge</b>, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao, Ying Shan, Feng Zheng
										</div>
										<div class="resume-degree-org text-muted">
											[<a href = "https://arxiv.org/abs/2305.02677" target="_blank">Tech Report</a>]
											[<a href = "https://huggingface.co/spaces/TencentARC/Caption-Anything" target="_blank">Demo</a>]
											[<a href = "https://github.com/ttengwang/Caption-Anything" target="_blank">Code</a>]
											<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ttengwang/Caption-Anything?style=social">
										</div>
									</li> -->
									<!-- <li>
										<div class="resume-degree font-weight-bold">
											TagGPT: Large Language Models are Zero-shot Multimodal Taggers
										</div>
										<font color="#CC0033">
											TagGPT is a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion.
										</font>
										<div class="resume-degree-org text-muted">
											Chen Li, <b>Yixiao Ge</b>, Jiayong Mao, Dian Li, Ying Shan
										</div>
										<div class="resume-degree-org text-muted">
											[<a href = "https://arxiv.org/abs/2304.03022" target="_blank">Tech Report</a>]
											[<a href = "https://huggingface.co/spaces/TencentARC/TagGPT" target="_blank">Demo</a>]
											[<a href = "https://github.com/TencentARC/TagGPT" target="_blank">Code</a>]
											<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TagGPT?style=social">
										</div>
									</li> -->

											<!-- <li>
												<div class="resume-degree font-weight-bold">
													Revitalize Region Feature for Democratizing Video-Language Pre-training
												</div>
												<div class="resume-degree-org text-muted">
													Guanyu Cai, <b>Yixiao Ge</b>, Alex Jinpeng Wang, Rui Yan, Xudong Lin, Ying Shan, Lianghua He, Xiaohu Qie, Jianping Wu, Mike Zheng Shou
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">Tech report, 2022</font>
													[<a href = "https://arxiv.org/abs/2203.07720" target="_blank">Paper</a>]
													[<a href = "https://github.com/CuthbertCai/DemoVLP" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CuthbertCai/DemoVLP?style=social">
												</div>
											</li> -->

											<!-- <li>
												<div class="resume-degree font-weight-bold">
													Privacy-Preserving Model Upgrades with Bidirectional Compatible Training in Image Retrieval
												</div>
												<div class="resume-degree-org text-muted">
													Shupeng Su*, Binjie Zhang*, <b>Yixiao Ge<sup>#</sup></b>, Xuyuan Xu, Yexin Wang, Chun Yuan, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">Tech report, 2022</font>
													[<a href = "https://arxiv.org/abs/2204.13919" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/OpenCompatible" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/OpenCompatible?style=social">
												</div>
											</li> -->


										<!-- 	<li>
												<div class="resume-degree font-weight-bold">
													Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification
												</div>
												<div class="resume-degree-org text-muted">
													<b>Yixiao Ge</b>, Xiao Zhang, Ching Lam Choi, Ka Chun Cheung, Peipei Zhao, Feng Zhu, Xiaogang Wang, Rui Zhao, Hongsheng Li
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">Tech report, 2021</font>
													[<a href = "./projects/bake.html" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2104.13298" target="_blank">Paper</a>]
													[<a href = "https://github.com/yxgeee/BAKE" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yxgeee/BAKE?style=social">
												</div>
											</li> -->
										</ul>

										<p>
										<font color="#39AB56"><b>2023:</b></font>
										</p>
										<ul class="resume-list" style="list-style: outside;" >

											<li>
												<div class="resume-degree font-weight-bold">
													GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction
												</div>
												<!-- <font color="#CC0033">
													We for the first time enable Vicuna-13B to use visual models via self-instruct tuning. The system can be deployed on local machines without APIs.
												</font> -->
												<div class="resume-degree-org text-muted">
													Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, <b>Yixiao Ge</b>, Xiu Li, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">NeurIPS, 2023</font>
													[<a href = "https://gpt4tools.github.io/" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2305.18752" target="_blank">Paper</a>]
													[<a href = "https://huggingface.co/spaces/stevengrove/GPT4Tools" target="_blank">Demo</a>]
													[<a href = "https://github.com/AILab-CVC/GPT4Tools" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/GPT4Tools?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
														Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models
												</div>
												<div class="resume-degree-org text-muted">
													Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, <b>Yixiao Ge</b>, Ying Shan, Mike Zheng Shou
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">NeurIPS, 2023</font>
													[<a href = "https://showlab.github.io/Mix-of-Show/" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2305.18292" target="_blank">Paper</a>]
													<!-- [<a href = "https://huggingface.co/spaces/stevengrove/GPT4Tools" target="_blank">Demo</a>] -->
													[<a href = "https://github.com/TencentARC/Mix-of-Show" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Mix-of-Show?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
														Meta-Adapter: An Online Few-shot Learner for Vision-Language Model
												</div>
												<div class="resume-degree-org text-muted">
													Cheng Cheng, Lin Song, Ruoyi Xue, Hang Wang, Hongbin Sun, <b>Yixiao Ge</b>, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">NeurIPS, 2023</font>
													<!-- [<a href = "https://gpt4tools.github.io/" target="_blank">Project</a>] -->
													[<a href = "" target="_blank">Paper (Coming soon)</a>]
													<!-- [<a href = "https://huggingface.co/spaces/stevengrove/GPT4Tools" target="_blank">Demo</a>] -->
													<!-- [<a href = "https://github.com/AILab-CVC/GPT4Tools" target="_blank">Code</a>] -->
													<!-- <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/GPT4Tools?style=social"> -->
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation
												</div>
												<!-- <font color="#CC0033">
													Given a video-text pair as input, our method, Tune-A-Video, fine-tunes a pre-trained text-to-image diffusion model for text-to-video generation.
												</font> -->
												<div class="resume-degree-org text-muted">
													Jay Zhangjie Wu, <b>Yixiao Ge</b>, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICCV, 2023</font>
													[<a href = "https://tuneavideo.github.io/" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2212.11565" target="_blank">Paper</a>]
													[<a href = "https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-inference" target="_blank">Demo</a>]
													[<a href = "https://github.com/showlab/Tune-A-Video" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/showlab/Tune-A-Video?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Exploring Model Transferability through the Lens of Potential Energy
												</div>
												<div class="resume-degree-org text-muted">
													Xiaotong Li, Zixuan Hu, <b>Yixiao Ge</b>, Ying Shan, Lingyu Duan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICCV, 2023</font>
													[<a href = "https://arxiv.org/abs/2308.15074" target="_blank">Paper</a>]
													[<a href = "https://github.com/lixiaotong97/PED" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/lixiaotong97/PED?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													BoxSnake: Polygonal Instance Segmentation with Box Supervision
												</div>
												<div class="resume-degree-org text-muted">
													Rui Yang, Lin Song, <b>Yixiao Ge</b>, Xiu Li
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICCV, 2023</font>
													[<a href = "https://arxiv.org/abs/2303.11630" target="_blank">Paper</a>]
													[<a href = "https://github.com/Yangr116/BoxSnake" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Yangr116/BoxSnake?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection
												</div>
												<div class="resume-degree-org text-muted">
													Yuxin Fang*, Shusheng Yang*, Shijie Wang*, <b>Yixiao Ge</b>, Ying Shan, Xinggang Wang
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICCV, 2023</font>
													[<a href = "https://arxiv.org/abs/2204.02964" target="_blank">Paper</a>]
													[<a href = "https://github.com/hustvl/MIMDet" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/hustvl/MIMDet?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Binary Embedding-based Retrieval at Tencent
												</div>
												<div class="resume-degree-org text-muted">
													Yukang Gan*, <b>Yixiao Ge*</b>, Chang Zhou*, Shupeng Su, Zhouchuan Xu, Xuyuan Xu, Quanchao Hui, Xiang Chen, Yexin Wang, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">KDD, 2023</font>
													[<a href = "https://arxiv.org/abs/2302.08714" target="_blank">Paper</a>]
													[<a href = "https://github.com/ganyk/BEBR" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ganyk/BEBR?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													π-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation
												</div>
												<div class="resume-degree-org text-muted">
													Chengyue Wu, Teng Wang, <b>Yixiao Ge<sup>#</sup></b>, Zeyu Lu, Ruisong Zhou, Ying Shan, Ping Luo
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICML, 2023</font>
													[<a href = "https://arxiv.org/abs/2304.14381" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/pi-Tuning" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/pi-Tuning?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Accelerating Vision-Language Pretraining with Free Language Modeling
												</div>
												<div class="resume-degree-org text-muted">
													Teng Wang, <b>Yixiao Ge</b>, Feng Zheng, Ran Cheng, Ying Shan, Xiaohu Qie, Ping Luo
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2023</font>
													[<a href = "https://arxiv.org/abs/2303.14038" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/FLM" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/FLM?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Masked Visual Reconstruction in Language Semantic Space
												</div>
												<div class="resume-degree-org text-muted">
													Shusheng Yang, <b>Yixiao Ge<sup>#</sup></b>, Kun Yi, Dian Li, Ying Shan, Xiaohu Qie, Xinggang Wang<sup>#</sup>
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2023</font>
													[<a href = "https://arxiv.org/abs/2301.06958" target="_blank">Paper</a>]
													[<a href = "https://github.com/hustvl/RILS" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/hustvl/RILS?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Learning Transferable Spatiotemporal Representations from Natural Script Knowledge
												</div>
												<div class="resume-degree-org text-muted">
													Ziyun Zeng*, Yuying Ge*, Xihui Liu, Bin Chen<sup>#</sup>, Ping Luo, Shu-Tao Xia, <b>Yixiao Ge<sup>#</sup></b>
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2023</font>
													[<a href = "https://arxiv.org/abs/2209.15280" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/TVTS" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TVTS?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													All in One: Exploring Unified Video-Language Pre-training
												</div>
												<div class="resume-degree-org text-muted">
													Alex Jinpeng Wang, <b>Yixiao Ge</b>, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Zheng Shou
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2023</font>
													[<a href = "https://arxiv.org/abs/2203.07303" target="_blank">Paper</a>]
													[<a href = "https://github.com/showlab/all-in-one" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/showlab/all-in-one?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Masked Image Modeling with Denoising Contrast
												</div>
												<div class="resume-degree-org text-muted">
													Kun Yi*, <b>Yixiao Ge*<sup>#</sup></b>, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, Xiaohu Qie
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICLR, 2023</font>
													[<a href = "https://openreview.net/pdf?id=1fZd4owfJP6" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/ConMIM" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/ConMIM?style=social">
												</div>
											</li>
											<li>
												<div class="resume-degree font-weight-bold">
													Darwinian Model Upgrades: Model Evolving with Selective Compatibility
												</div>
												<div class="resume-degree-org text-muted">
													Binjie Zhang*, Shupeng Su*, <b>Yixiao Ge<sup>#</sup></b>, Xuyuan Xu, Yexin Wang, Chun Yuan, Mike Zheng Shou, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">AAAI, 2023</font>
													[<a href = "https://arxiv.org/abs/2210.06954" target="_blank">Paper</a>]
													<!-- [<a href = "https://arxiv.org/abs/2204.13919" target="_blank">Paper (v1)</a>] -->
													<!-- [<a href = "https://github.com/TencentARC/OpenCompatible" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/OpenCompatible?style=social"> -->
												</div>
											</li>
											<li>
												<div class="resume-degree font-weight-bold">
													Video-Text Pre-training with Learned Regions
												</div>
												<div class="resume-degree-org text-muted">
													Rui Yan, Mike Zheng Shou, <b>Yixiao Ge</b>, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, Jinhui Tang
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">AAAI, 2023</font>
													[<a href = "https://arxiv.org/abs/2112.01194" target="_blank">Paper</a>]
													[<a href = "https://github.com/showlab/Region_Learner" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/showlab/Region_Learner?style=social">
												</div>
											</li>
										</ul>

										<p>
										<font color="#39AB56"><b>2022:</b></font>
										</p>
										<ul class="resume-list" style="list-style: outside;" >


											<li>
												<div class="resume-degree font-weight-bold">
													MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval
												</div>
												<div class="resume-degree-org text-muted">
													Yuying Ge, <b>Yixiao Ge</b>, Xihui Liu, Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ECCV, 2022</font>
													[<a href = "https://arxiv.org/abs/2204.12408" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/MCQ/tree/main/MILES" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Not All Models Are Equal: Predicting Model Transferability in a Self-challenging Fisher Space
												</div>
												<div class="resume-degree-org text-muted">
													Wenqi Shao<sup>#</sup>, Xun Zhao, <b>Yixiao Ge<sup>#</sup></b>, Zhaoyang Zhang, Lei Yang, Xiaogang Wang, Ying Shan, Ping Luo
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ECCV, 2022</font>
													[<a href = "https://arxiv.org/abs/2207.03036" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/SFDA" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/SFDA?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													mc-BEiT: Multi-choice Discretization for Image BERT Pre-training
												</div>
												<div class="resume-degree-org text-muted">
													Xiaotong Li, <b>Yixiao Ge</b>, Kun Yi, Zixuan Hu, Ying Shan, Lingyu Duan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ECCV, 2022</font>
													[<a href = "https://arxiv.org/abs/2203.15371" target="_blank">Paper</a>]
													[<a href = "https://github.com/lixiaotong97/mc-BEiT" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/lixiaotong97/mc-BEiT?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Towards Universal Backward-Compatible Representation Learning
												</div>
												<div class="resume-degree-org text-muted">
													Binjie Zhang, <b>Yixiao Ge<sup>#</sup></b>, Yantao Shen, Shupeng Su, Fanzi Wu, Chun Yuan<sup>#</sup>, Xuyuan Xu, Yexin Wang, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">IJCAI, 2022 <b>(Long oral)</b></font>
													<!-- [<a href = "https://geyuying.github.io/MCQ.html" target="_blank">Project</a>] -->
													[<a href = "https://arxiv.org/abs/2203.01583" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/OpenCompatible" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/OpenCompatible?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Bridging Video-text Retrieval with Multiple Choice Questions
												</div>
												<div class="resume-degree-org text-muted">
													Yuying Ge, <b>Yixiao Ge</b>, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, Ping Luo
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2022 <b>(Oral)</b></font>
													<!-- [<a href = "https://geyuying.github.io/MCQ.html" target="_blank">Project</a>] -->
													[<a href = "https://arxiv.org/abs/2201.04850" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/MCQ" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Object-aware Video-language Pre-training for Retrieval
												</div>
												<div class="resume-degree-org text-muted">
													Alex Jinpeng Wang, <b>Yixiao Ge</b>, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, Mike Zheng Shou
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2022</font>
													<!-- [<a href = "./projects/bake.html" target="_blank">Project</a>] -->
													[<a href = "https://arxiv.org/abs/2112.00656" target="_blank">Paper</a>]
													[<a href = "https://github.com/FingerRec/OA-Transformer" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/FingerRec/OA-Transformer?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Hot-Refresh Model Upgrades with Regression-Alleviating Compatible Training in Image Retrieval
												</div>
												<div class="resume-degree-org text-muted">
													Binjie Zhang, <b>Yixiao Ge<sup>#</sup></b>, Yantao Shen, Yu Li, Chun Yuan<sup>#</sup>, Xuyuan Xu, Yexin Wang, Ying Shan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICLR, 2022</font>
													<!-- [<a href = "./projects/bake.html" target="_blank">Project</a>] -->
													[<a href = "https://openreview.net/pdf?id=HTp-6yLGGX" target="_blank">Paper</a>]
													[<a href = "https://github.com/TencentARC/OpenCompatible" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/OpenCompatible?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Dynamic Token Normalization Improves Vision Transformer
												</div>
												<div class="resume-degree-org text-muted">
													Wenqi Shao, <b>Yixiao Ge</b>, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, Ping Luo
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICLR, 2022</font>
													<!-- [<a href = "./projects/bake.html" target="_blank">Project</a>] -->
													[<a href = "https://openreview.net/pdf?id=f9MHpAGUyMn" target="_blank">Paper</a>]
													[<a href = "https://github.com/wqshao126/DTN" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/wqshao126/DTN?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Uncertainty Modeling for Out-of-Distribution Generalization
												</div>
												<div class="resume-degree-org text-muted">
													Xiaotong Li, Yongxing Dai, <b>Yixiao Ge</b>, Jun Liu, Ying Shan, Lingyu Duan
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICLR, 2022</font>
													<!-- [<a href = "./projects/bake.html" target="_blank">Project</a>] -->
													[<a href = "https://openreview.net/pdf?id=6HN7LHyzGgC" target="_blank">Paper</a>]
													[<a href = "https://github.com/lixiaotong97/DSU" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/lixiaotong97/DSU?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID
												</div>
												<div class="resume-degree-org text-muted">
													<b>Yixiao Ge</b>, Feng Zhu, Dapeng Chen, Rui Zhao, Xiaogang Wang, Hongsheng Li
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">IEEE TNNLS, 2022</font>
													[<a href = "./projects/sda.html" target="_blank">Project</a>]
													[<a href = "https://arxiv.org/abs/2003.06650" target="_blank">Paper</a>]
													<!-- [<a href = "https://github.com/yxgeee/SDA" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yxgeee/SDA?style=social"> -->
												</div>
											</li>

										</ul>

										<p>
										<font color="#39AB56"><b>2021:</b></font>
										</p>
										<ul class="resume-list" style="list-style: outside;" >

											<li>
												<div class="resume-degree font-weight-bold">
													Progressive Correspondence Pruning by Consensus Learning
												</div>
												<div class="resume-degree-org text-muted">
													Chen Zhao*, <b>Yixiao Ge*</b>, Feng Zhu, Rui Zhao, Hongsheng Li, Mathieu Salzmann <!-- (*Co-first Authors) -->
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">ICCV, 2021</font>
													[<a href = "https://sailor-z.github.io/projects/CLNet" target="_blank">Project</a>]
													[<a href = "https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Progressive_Correspondence_Pruning_by_Consensus_Learning_ICCV_2021_paper.pdf" target="_blank">Paper</a>]
													[<a href = "https://github.com/sailor-z/CLNet" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/sailor-z/CLNet?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Online Pseudo Label Generation by Hierarchical Cluster Dynamics for Adaptive Person Re-identification
												</div>
												<div class="resume-degree-org text-muted">
													Yi Zheng, Shixiang Tang, Guolong Teng, <b>Yixiao Ge</b>, Kaijian Liu, Donglian Qi, Jing Qin, Dapeng Chen
												</div>
												<div class="resume-degree-org text-muted">
														<font color="#39AB56">ICCV, 2021</font>
														[<a href = "https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Online_Pseudo_Label_Generation_by_Hierarchical_Cluster_Dynamics_for_Adaptive_ICCV_2021_paper.pdf" target="_blank">Paper</a>]
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification
												</div>
												<div class="resume-degree-org text-muted">
													Xiao Zhang*, <b>Yixiao Ge*</b>, Yu Qiao, Hongsheng Li
													<!-- (*Co-first Authors) -->
												</div>
												<div class="resume-degree-org text-muted">
														<font color="#39AB56">CVPR, 2021</font> [<a href = "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Refining_Pseudo_Labels_With_Clustering_Consensus_Over_Generations_for_Unsupervised_CVPR_2021_paper.pdf" target="_blank">Paper</a>]
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network
												</div>
												<div class="resume-degree-org text-muted">
													Rui Liu, <b>Yixiao Ge</b>, Ching Lam Choi, Xiaogang Wang, Hongsheng Li
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">CVPR, 2021</font>
													[<a href = "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_DivCo_Diverse_Conditional_Image_Synthesis_via_Contrastive_Generative_Adversarial_Network_CVPR_2021_paper.pdf" target="_blank">Paper</a>]
													[<a href = "https://github.com/ruiliu-ai/DivCo" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ruiliu-ai/DivCo?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Mutual CRF-GNN Network for Few-shot Learning
												</div>
												<div class="resume-degree-org text-muted">
													Shixiang Tang, Dapeng Chen, Lei Bai, Kaijian Liu, <b>Yixiao Ge</b>, Wanli Ouyang
												</div>
												<div class="resume-degree-org text-muted">
														<font color="#39AB56">CVPR 2021 </font> [<a href = "https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf" target="_blank">Paper</a>]
												</div>
											</li>
										</ul>

										<p>
										<font color="#39AB56"><b>2020:</b></font>
										</p>
										<ul class="resume-list" style="list-style: outside;" >

											<li>
												<div class="resume-degree font-weight-bold">
													Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID
												</div>
												<div class="resume-degree-org text-muted">
													<b>Yixiao Ge</b>, Feng Zhu, Dapeng Chen, Rui Zhao, Hongsheng Li
												</div>
												<div class="resume-degree-org text-muted">
														<font color="#39AB56">NeurIPS, 2020</font>
													[<a href = "./projects/spcl.html" target="_blank">Project</a>]
													[<a href = "https://proceedings.neurips.cc/paper/2020/file/821fa74b50ba3f7cba1e6c53e8fa6845-Paper.pdf" target="_blank">Paper</a>]
													[<a href = "https://github.com/yxgeee/SpCL" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yxgeee/SpCL?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Self-supervising Fine-grained Region Similarities for Large-scale Image Localization
												</div>
												<div class="resume-degree-org text-muted">
													<b>Yixiao Ge</b>, Haibo Wang, Feng Zhu, Rui Zhao, Hongsheng Li
												</div>
												<div class="resume-degree-org text-muted">
														<font color="#39AB56">ECCV, 2020 <b>(Spotlight)</b></font>
													[<a href = "./projects/sfrs.html" target="_blank">Project</a>]
													[<a href = "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490358.pdf" target="_blank">Paper</a>]
													[<a href = "https://github.com/yxgeee/OpenIBL" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yxgeee/OpenIBL?style=social">
												</div>
											</li>

											<li>
												<div class="resume-degree font-weight-bold">
													Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification
												</div>
												<div class="resume-degree-org text-muted">
													<b>Yixiao Ge</b>, Dapeng Chen, Hongsheng Li
												</div>
												<div class="resume-degree-org text-muted">
														<font color="#39AB56">ICLR, 2020</font>
													[<a href = "./projects/mmt.html" target="_blank">Project</a>]
													[<a href = "https://openreview.net/pdf?id=rJlnOhVYPS" target="_blank">Paper</a>]
													[<a href = "https://github.com/yxgeee/MMT" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yxgeee/MMT?style=social">
												</div>
											</li>
										</ul>

										<p>
										<font color="#39AB56"><b>Before 2020:</b></font>
										</p>
										<ul class="resume-list" style="list-style: outside;" >

											<li>
												<div class="resume-degree font-weight-bold">
													FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification
												</div>
												<div class="resume-degree-org text-muted">
													<b>Yixiao Ge*</b>, Zhuowan Li*, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, Hongsheng Li
													<!-- (*Co-first Authors) -->
												</div>
												<div class="resume-degree-org text-muted">
													<font color="#39AB56">NeurIPS, 2018</font>
													[<a href = "./projects/fdgan.html" target="_blank">Project</a>]
													[<a href = "http://papers.nips.cc/paper/7398-fd-gan-pose-guided-feature-distilling-gan-for-robust-person-re-identification.pdf" target="_blank">Paper</a>]
													[<a href = "https://github.com/yxgeee/FD-GAN" target="_blank">Code</a>]
													<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yxgeee/FD-GAN?style=social">
												</div>
											</li>

										</ul>
									</div>
								</div><!--//item-->



							</section><!--//project-section-->
						</div><!--//resume-main-->
						<aside class="resume-aside col-12 col-lg-4 col-xl-3 px-lg-4 pb-lg-4">
									<section class="education-section py-3">
										<h3 class="text-uppercase resume-section-heading mb-4">Education</h3>
										<ul class="list-unstyled resume-education-list">
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[2018-2021] Ph.D.,</div>
												<div class="resume-degree-org text-muted"><b>MMLab, The Chinese University of Hong Kong</b></div>
											</li>
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[2013-2017] B.Eng.,</div>
												<div class="resume-degree-org text-muted"><b>Huazhong University of Science and Technology</b></div>
											</li>
										</ul>
									</section><!--//education-section-->
									<section class="education-section py-3">
										<h3 class="text-uppercase resume-section-heading mb-4">Experience</h3>
										<ul class="list-unstyled resume-education-list">
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[2021-present]</div>
												<div class="resume-degree font-weight-bold">Senior Researcher,</div>
												<div class="resume-degree-org text-muted"><b>Tencent</b></div>
											</li>
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[2019-2020]</div>
												<div class="resume-degree font-weight-bold">Research Intern,</div>
												<div class="resume-degree-org text-muted"><b>SenseTime Research</b></div>
											</li>
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[2017-2018]</div>
												<div class="resume-degree font-weight-bold">Research Assistant,</div>
												<div class="resume-degree-org text-muted"><b>MMLab, The Chinese University of Hong Kong</b></div>
											</li>
										</ul>
									</section><!--//education-section-->

									<!-- <section class="education-section py-3">
										<h3 class="text-uppercase resume-section-heading mb-4">Awards</h3>
										<ul class="list-unstyled resume-awards-list">
											<li class="mb-3">
												<div class="font-weight-bold">Future Star, SenseTime Group Limited. 2020</div>
											</li>
											<li class="mb-3">
												<div class="font-weight-bold">Outstanding PhD Thesis, CSIG</div>
											</li>
											<li class="mb-3">
												<div class="font-weight-bold">Wen-Tsun Wu Award, 2019</div>
											</li>
										</ul>
									</section> --><!--//education-section-->
									<section class="skills-section py-3">
										<h3 class="text-uppercase resume-section-heading mb-4">Reviewers</h3>
										<ul class="list-unstyled resume-lang-list">
											<ul class="list-unstyled pb-2">
												(Top-tier conferences)
												<li> <b>NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV, AAAI</b> </li>
												<!-- <li> <b>NeurIPS</b> 2020, 2021, 2022</li>
												<li> <b>ICLR</b> 2021, 2022, 2023</li>
												<li> <b>ICML</b> 2021, 2022</li>
												<li><b>CVPR</b> 2021, 2022</li>
												<li><b>ICCV</b> 2021</li>
												<li><b>ECCV</b> 2022</li> -->
												<br>
												(Top-tier journals)
												<li> <b>IEEE TPAMI, IJCV, IEEE TIP, IEEE TCSVT, IEEE TMM, Neurocomputing</b></li>
											</ul>
										</ul>
									</section><!--//certificates-section-->


									<!-- <section class="education-section py-3">
										<h3 class="text-uppercase resume-section-heading mb-4">Invited Talks</h3>
										<ul class="list-unstyled resume-education-list">
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[Jul 2022]</div>
												<div class="resume-degree-org text-muted"><b>"Video-language Pre-training and Representation Learning" at TAIC. <a href="https://mp.weixin.qq.com/s/2DMVFzKhjrhp9bf0tP7CIQ" target="_blank">[Poster]</a></b></div>
											</li>
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[Apr 2022]</div>
												<div class="resume-degree-org text-muted"><b>"Representation Learning at ARC Lab" at ICLR. <a href="https://iclr.cc/ExpoConferences/2022/Sponsorpage?id=155" target="_blank">[Web]</a></b></div>
											</li>
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[Dec 2020]</div>
												<div class="resume-degree-org text-muted"><b>"Analysis and Development of OpenUnReID Codebase" at ZhiDX (智东西). <a href="https://appoSCMf8kb5033.h5.xeknow.com/st/9k0kMekYC" target="_blank">[Video]</a></b></div>
											</li>
											<li class="mb-3">
												<div class="resume-degree font-weight-bold">[Nov 2020]</div>
												<div class="resume-degree-org text-muted"><b>"Unsupervised and Domain Adaptive Object Re-identification" at TechBeat (将门). <a href="https://www.techbeat.net/talk-info?id=456" target="_blank">[Video]</a></b></div>
											</li>

										</ul>
									</section> -->

									<section class="skills-section py-3">
										<h3 class="text-uppercase resume-section-heading mb-4">Collaborators</h3>
										<ul class="list-unstyled resume-lang-list">
											<ul class="list-unstyled pb-2">
												<li> <a href = "https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan</a></li>
												<li> <a href = "https://geyuying.github.io/" target="_blank">Yuying Ge</a></li>
												<li> <a href = "https://scholar.google.com/citations?user=6xtzo4AAAAAJ" target="_blank">Kun Yi</a></li>
												<li> <a href = "http://linsong.info/" target="_blank">Lin Song</a></li>
												<li> <a href = "https://scholar.google.com/citations?user=5fU_DtEAAAAJ" target="_blank">Chen Li</a></li>
												<li> <a href = "https://dblp.org/pid/179/2341.html" target="_blank">Yukang Gan</a></li>
												<!-- <li> <a href = "https://scholar.google.com/citations?user=z4Nt3AQAAAAJ" target="_blank">Shupeng Su</a></li> -->
												<li> <a href = "https://dingxiaohan.xyz/" target="_blank">Xiaohan Ding</a></li>
												<!-- <li> <a href = "https://scholar.google.com/citations?user=6FsgWBMAAAAJ" target="_blank">Zhan Tong</a></li> -->
												<li> <a href = "" target="_blank">Sijie Zhao</a></li>
												<li> <a href = "http://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a></li>
												<li> <a href = "http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Xiaogang Wang</a></li>
												<li> <a href = "http://zhaorui.xyz/" target="_blank">Rui Zhao</a></li>
												<li> <a href = "https://scholar.google.com/citations?user=-Wpd7FcAAAAJ&hl=en" target="_blank">Dapeng Chen</a></li>
												<li> <a href = "https://zhufengx.github.io/" target="_blank">Feng Zhu</a></li>
											</ul>
										</ul>
									</section>

							<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=59b2dz2jgmo&amp;m=2&amp;c=007eff&amp;cr1=00ff6c&amp;sx=0" async="async"></script> -->
							<!-- <div style="width: 50%;">
        					<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=l25fbN5OAQusW9PW4IQZ6SVRyZFmlfUhwNbkjV8LeBc&co=333333&cmo=3acc3a&cmn=ff5353&ct=e7e7e7'></script>
        					<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=l25fbN5OAQusW9PW4IQZ6SVRyZFmlfUhwNbkjV8LeBc"></script>
        					</div> -->
        					<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=220&t=tt&d=l25fbN5OAQusW9PW4IQZ6SVRyZFmlfUhwNbkjV8LeBc&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353"></script>

								</aside><!--//resume-aside-->
							</div><!--//row-->
						</div><!--//resume-body-->

					</article>

				</div><!--//container-->

				<!-- <footer class="footer text-center py-4">
					&lt;!&ndash;/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */&ndash;&gt;
					<small class="copyright text-muted">Designed with <i class="fas fa-heart"></i> by <a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small>
				</footer> -->

			</div><!--//main-wrapper-->


</body>
</html>
